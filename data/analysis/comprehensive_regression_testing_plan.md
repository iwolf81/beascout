# Comprehensive Regression Testing Implementation Plan

## Executive Summary
This plan combines the technical approach from `regression_testing_plan.md` with the implementation requirements from `regression_testing_10Sep2025.md` to create a complete regression testing framework for the BeAScout pipeline.

## Current State Analysis

### Scripts Currently Supporting CLI Arguments
- ✅ `three_way_validator.py` - Full argparse support (--key-three, --scraped-data, --output)
- ✅ `generate_commissioner_report.py` - Argparse support (--key-three)
- ✅ `generate_unit_emails.py` - Positional arguments for inputs
- ✅ `process_full_dataset.py` - Takes scraped data directory as argument
- ⚠️ `multi_zip_scraper.py` - Basic sys.argv (test/full modes only)

### Missing Input File Generation Documentation
- ❌ `hne_council_zipcodes.json` generation not documented in OPERATIONAL_WORKFLOW.md
- ❌ Complete input file generation workflow missing

## Phase 1: Input File Generation Infrastructure

### Task 1.1: Document Missing Input File Generation
**File**: Update `OPERATIONAL_WORKFLOW.md`
**Action**: Add complete section for input file generation including:
```bash
# Generate HNE Council zip codes file
python src/pipeline/core/hne_towns.py
# Output: data/zipcodes/hne_council_zipcodes.json
```

### Task 1.2: Enhance Multi-Zip Scraper CLI Support
**File**: `src/pipeline/acquisition/multi_zip_scraper.py`
**Action**: Add argparse support with:
- `--zip-codes-file` (default: `data/zipcodes/hne_council_zipcodes.json`)
- `--mode` (choices: test, full, custom)
- `--output-dir` (default: `data/scraped/YYYYMMDD_HHMMSS/`)

### Task 1.3: Enhance HNE Towns Generator CLI Support  
**File**: `src/pipeline/core/hne_towns.py`
**Action**: Add argparse support with:
- `--output-file` (default: `data/zipcodes/hne_council_zipcodes.json`)
- `--council-name` (default: "Heart of New England", for future multi-council support)

## Phase 2: Complete Reference Library Creation

### Task 2.1: Audit All Generated Files
**Action**: Identify all files generated by the pipeline:
- **Acquisition**: `data/scraped/*/beascout_*.html`, `data/scraped/*/joinexploring_*.html`
- **Processing**: `data/raw/all_units_comprehensive_scored.json`
- **Analysis**: `data/output/three_way_validation_results.json`, Excel reports, unit emails
- **Debug**: All debug logs in `data/debug/`
- **Input**: `data/zipcodes/hne_council_zipcodes.json`

### Task 2.2: Create Reference Data Structure
**Structure**: `tests/reference/`
```
tests/reference/
├── input/
│   ├── zipcodes/
│   │   └── hne_council_zipcodes.json
│   └── key_three/
│       ├── anonymized_key_three.xlsx
│       └── anonymized_key_three.json
├── scraped/
│   ├── beascout_01720.html
│   ├── joinexploring_01720.html
│   └── [all test scraped files]
├── processed/
│   └── all_units_comprehensive_scored.json
├── analyzed/
│   ├── three_way_validation_results.json
│   ├── BeAScout_Quality_Report_reference.xlsx
│   └── unit_emails/
│       └── [reference email files]
└── debug/
    ├── unit_identifier_debug_scraped_reference.log
    └── cross_reference_validation_debug_reference.log
```

### Task 2.3: Generate Complete Reference Dataset
**Action**: Run full pipeline with test data to create reference outputs for comparison

## Phase 3: CLI Arguments Enhancement

### Task 3.1: Standardize All Pipeline Scripts
**Scripts requiring updates**:

#### `multi_zip_scraper.py`
```python
parser.add_argument('--zip-codes-file', default='data/zipcodes/hne_council_zipcodes.json')
parser.add_argument('--mode', choices=['test', 'full'], default='test')
parser.add_argument('--output-dir', default=None)  # Auto-generate timestamp
```

#### `process_full_dataset.py` 
```python
parser.add_argument('scraped_dir', help='Directory containing scraped HTML files')
parser.add_argument('--output-file', default='data/raw/all_units_comprehensive_scored.json')
```

#### `hne_towns.py`
```python
parser.add_argument('--output-file', default='data/zipcodes/hne_council_zipcodes.json')
```

### Task 3.2: Verify Existing CLI Arguments
**Action**: Test that existing argparse implementations work correctly with test data paths

## Phase 4: Master Scripts Creation

### Task 4.1: Create Real Data Master Script
**File**: `scripts/run_pipeline_real_data.sh`
**Content**: Complete pipeline execution using real data inputs
```bash
#!/bin/bash
# Generate HNE zip codes
python src/pipeline/core/hne_towns.py

# Scrape real data  
python src/pipeline/acquisition/multi_zip_scraper.py --mode full

# Process scraped data
SCRAPED_DIR=$(ls -td data/scraped/*/ | head -n1)
python src/pipeline/processing/process_full_dataset.py "$SCRAPED_DIR"

# Convert Key Three to JSON
python src/dev/tools/convert_key_three_to_json.py "data/input/Key 3 08-22-2025.xlsx"

# Three-way validation
python src/pipeline/analysis/three_way_validator.py --key-three "data/input/Key 3 08-22-2025.json"

# Generate reports
python src/pipeline/analysis/generate_commissioner_report.py
python src/pipeline/analysis/generate_unit_emails.py data/raw/all_units_comprehensive_scored.json "data/input/Key 3 08-22-2025.xlsx"
```

### Task 4.2: Create Test Data Master Script  
**File**: `scripts/run_pipeline_test_data.sh`
**Content**: Complete pipeline execution using reference test data
```bash
#!/bin/bash
# Process test scraped data
python src/pipeline/processing/process_full_dataset.py tests/reference/scraped/

# Three-way validation with test data
python src/pipeline/analysis/three_way_validator.py --key-three tests/reference/key_three/anonymized_key_three.json

# Generate reports with test data
python src/pipeline/analysis/generate_commissioner_report.py --key-three tests/reference/key_three/anonymized_key_three.json
python src/pipeline/analysis/generate_unit_emails.py data/raw/all_units_comprehensive_scored.json tests/reference/key_three/anonymized_key_three.xlsx
```

## Phase 5: Regression Testing Framework

### Task 5.1: Excel Comparison Tool
**File**: `tests/tools/compare_excel_files.py`
**Functionality**:
- Export Excel sheets to CSV with normalized content
- Filter dynamic timestamps, file paths, generated IDs
- Compare business-critical data: unit counts, quality scores, recommendations
- Generate detailed diff reports

### Task 5.2: Email Comparison Tool  
**File**: `tests/tools/compare_email_files.py`
**Functionality**:
- Strip timestamps, file paths, generated identifiers
- Compare core content: unit details, recommendations, contact information  
- Generate structured comparison reports

### Task 5.3: JSON Comparison Tool
**File**: `tests/tools/compare_json_files.py` 
**Functionality**:
- Deep comparison with dynamic content filtering
- Structured diff reporting for complex nested data
- Business logic validation (unit counts, quality scores)

### Task 5.4: Master Regression Test Runner
**File**: `tests/run_regression_tests.py`
**Functionality**:
```python
def run_regression_tests():
    # 1. Run test pipeline to generate new outputs
    subprocess.run(['bash', 'scripts/run_pipeline_test_data.sh'])
    
    # 2. Compare all generated files against reference
    results = []
    results.append(compare_json('data/raw/all_units_comprehensive_scored.json', 
                               'tests/reference/processed/all_units_comprehensive_scored.json'))
    results.append(compare_excel('data/output/reports/BeAScout_Quality_Report_*.xlsx',
                                'tests/reference/analyzed/BeAScout_Quality_Report_reference.xlsx'))
    results.append(compare_emails('data/output/unit_emails/', 
                                 'tests/reference/analyzed/unit_emails/'))
    
    # 3. Generate comprehensive test report
    generate_regression_report(results)
```

## Phase 6: Dynamic Content Normalization

### Task 6.1: Content Normalization Utilities
**File**: `tests/tools/normalize_content.py`
**Functions**:
```python
def normalize_timestamps(content):
    # Replace 2025-09-09_14:30:15 with <TIMESTAMP>
    
def normalize_file_paths(content): 
    # Replace /full/path/file.xlsx with <FILEPATH>/file.xlsx
    
def normalize_generated_ids(content):
    # Replace auto-generated identifiers with <GENERATED_ID>
    
def normalize_dates(content):
    # Replace September 9, 2025 with <DATE>
```

### Task 6.2: Business Logic Validation
**File**: `tests/tools/validate_business_logic.py`
**Functions**:
- Validate unit counts match expected ranges
- Verify quality score calculations
- Check district assignment consistency  
- Validate Key Three correlation accuracy

## Phase 7: Integration and Documentation Updates

### Task 7.1: Update OPERATIONAL_WORKFLOW.md
**Action**: Add complete input file generation section and regression testing commands

### Task 7.2: Create Regression Testing Documentation
**File**: `tests/README.md`
**Content**: Complete guide to running and interpreting regression tests

### Task 7.3: Integration with CI/CD
**File**: `.github/workflows/regression_tests.yml`
**Action**: Automated regression testing on pull requests

## Implementation Priority

1. **Phase 1** (Critical) - CLI enhancements and missing documentation
2. **Phase 2** (High) - Complete reference library creation  
3. **Phase 4** (High) - Master scripts for automated pipeline execution
4. **Phase 5** (Medium) - Regression testing framework
5. **Phase 3** (Medium) - CLI standardization completion
6. **Phase 6** (Low) - Advanced content normalization
7. **Phase 7** (Low) - Documentation and CI/CD integration

## Expected Outcomes

1. **100% Pipeline Coverage** - All generated files have regression test coverage
2. **Automated Testing** - Single command execution for full regression test suite
3. **Clear Documentation** - Complete operational workflow documentation
4. **Robust Comparison** - Business logic validation while ignoring presentation changes  
5. **Development Safety** - Catch functional regressions before deployment

This framework will enable confident development with comprehensive regression detection while being robust to expected dynamic content changes.